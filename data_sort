from cgi import test
from importlib.resources import path
from operator import index
from statistics import mode
import pandas as pd
import numpy as np
import datetime


###NTS: Just added a bunch of stations. Probablly need to QC them since I'm getting a lot of errors now.


data_parent_directory = "D:/Weather_Modeling/Neural Networks/TeleIndicesModel/TeleIndicesModel/Data/"
# parent_folder = os.path.dirname(path)
NAO_file_path = data_parent_directory + "nao.reanalysis.t10trunc.1948-present.txt"
PNA_file_path = data_parent_directory + "pna.reanalysis.t10trunc.1948-present.txt"
SOI_file_path = data_parent_directory + "SOIdata"
AO_file_path = data_parent_directory + "AOdata"
Sounding_filepath_1 = data_parent_directory + "CAM00071867-data.txt" # The PAS, UA, CAN
Sounding_filepath_2 = data_parent_directory + "CAM00071109-data.txt" # Port Hardy, UA, CAN
Sounding_filepath_3 = data_parent_directory + "USM00072206-data.txt" ###!!Jacksonville, FL. There's a lot of missing days. Needs to be QCed
Sounding_filepath_4 = data_parent_directory + "USM00072381-data.txt" ##!!Edwards AFB, CA. Small sample size, Do not use
Sounding_filepath_5 = data_parent_directory + "USM00072456-data.txt" #Topeka, KS
Sounding_filepath_6 = data_parent_directory + "USM00074494-data.txt" #Chatam, MA
Sounding_filepath_7 = data_parent_directory + "USM00072327-data.txt" #Nashville, TN
Sounding_filepath_8 = data_parent_directory + "USM00072662-data.txt" #Rapid City, SD
Sounding_filepath_9 = data_parent_directory + "USM00072363-data.txt" #Amarillo, TX
Sounding_filepath_10 = data_parent_directory + "USM00072572-data.txt" #Salt Lake City, UT
Sounding_filepath_11 = data_parent_directory + "USM00072694-data.txt" #Salem, OR. 
Sounding_filepath_12 = data_parent_directory + "USM00072698-data.txt" #!!Portland, OR. Only goes to 1972, Do not use
Sounding_filepath_13 = data_parent_directory + "CAM00071722-data.txt"
Sounding_filepath_14 = data_parent_directory + "CAM00071823-data.txt" #!!Starts at 1985, Do not use
Sounding_filepath_15 = data_parent_directory + "USM00072403-data.txt" #Sterling, VA
Sounding_filepath_16 = data_parent_directory + "USM00091165-data.txt" #Lihue, HI
Sounding_filepath_17 = data_parent_directory + "USM00072712-data.txt" #Caribou, ME, Dense data from 1979
Sounding_filepath_18 = data_parent_directory + "USM00072597-data.txt" #Medford, OR, Dense Data from 1962    
Sounding_filepath_19 = data_parent_directory + "USM00072476-data.txt" #Grand Junction, CO, Dense Data from 1969
Sounding_filepath_20 = data_parent_directory + "USM00072365-data.txt" #Albuquerque, NM, Dense Data from 1969
Sounding_filepath_21 = data_parent_directory + "USM00072274-data.txt" #Touscon, AZ, Dense Data from 1969
Sounding_filepath_22 = data_parent_directory + "USM00072317-data.txt" #Greensboro, NC, Dense Data from 1969

Station_data_file_path = data_parent_directory + "KNGU_station_data.txt" # KIAD_Station_data.txt  KSDF_station_data.csv KNGU_station_data.txt

Target_Value = "TMAX" #Target_Value is the parameter you want to predict. Can be 'TAVG', 'TMAX', 'TMIN'

observation_start_date = pd.to_datetime("1965-4-1") #for if the observation begins at any other time than the beginning of the dataset
target_forecast = 30
test_ratio = .1

### Section for selecting which months of the year to include in the dataset
### 
JJA = True
SON = True
DJF = True
MAM = True

def split_train(data, data_labels, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(shuffled_indices) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data[test_indices], data_labels[test_indices], data[train_indices], data_labels[train_indices]

def sounding_entry(filepath):
    with open(filepath) as f:
        sounding_lines = f.readlines()
    f.close

    seven_gph = []
    seven_temp = []
    seven_wd = []
    seven_ws = []
    five_gph = []
    five_temp = []
    five_wd = []
    five_ws = []
    two_gph = []
    two_temp = []
    two_wd = []
    two_ws = []

    date_index = []
    seven_skipped = False
    five_skipped = False
    two_skipped = False
    desired_sounding = False
    for _, line in enumerate(sounding_lines):
        date_line = line.split()            
                   
        if date_line[0][0] == "#" and date_line[4] == "12":
            # if skipped:             ## For error correction. If the previous sounding did not have the associated px level
            #     date_index.pop()    ## it will discard that date so dates and data remain aligned
                if  seven_skipped:
                    seven_gph.append(np.NAN)
                    seven_temp.append(np.NAN)
                    seven_wd.append(np.NAN)
                    seven_ws.append(np.NAN)
                if  five_skipped:
                    five_gph.append(np.NAN)
                    five_temp.append(np.NAN)
                    five_wd.append(np.NAN)
                    five_ws.append(np.NAN)
                if  two_skipped:
                    two_gph.append(np.NAN)
                    two_temp.append(np.NAN)
                    two_wd.append(np.NAN)
                    two_ws.append(np.NAN)   
                date = datetime.datetime(int(date_line[1]), int(date_line[2]), int(date_line[3]))
                date_index.append(date)
                desired_sounding = True
                seven_skipped = True
                five_skipped = True
                two_skipped = True
        elif date_line[0][0] == "#":
            desired_sounding = False
        if desired_sounding:
            line = line.replace("A", " ")
            line = line.replace("B", " ")
            # line = line.replace("\n", " ")
            line = line.split()
            if float(line[2]) == 70000:
                seven_gph.append(float(line[3])) #adds Geopotential height to the array
                seven_temp.append(float(line[4])) #adds Temperature to the array
                seven_wd.append(float(line[7])) #adds Wind Direction to the array
                seven_ws.append(float(line[8])) #adds Wind Speed to the array
                seven_skipped = False

            if float(line[2]) == 50000: 
                five_gph.append(float(line[3])) #adds Geopotential height to the array
                five_temp.append(float(line[4])) #adds Temperature to the array
                five_wd.append(float(line[7])) #adds Wind Direction to the array
                five_ws.append(float(line[8])) #adds Wind Speed to the array
                five_skipped = False

            if float(line[2]) == 20000: 
                two_gph.append(float(line[3])) #adds Geopotential height to the array
                two_temp.append(float(line[4])) #adds Temperature to the array
                two_wd.append(float(line[7])) #adds Wind Direction to the array
                two_ws.append(float(line[8])) #adds Wind Speed to the array 
                two_skipped = False
        
    date_index = pd.DatetimeIndex(date_index)
    return pd.DataFrame( {"700 Sounding GPH": seven_gph,"700 Sounding Temp": seven_temp,"700 Sounding WD": seven_wd,"700 Sounding WS": seven_ws,
                          "500 Sounding GPH": five_gph,"500 Sounding Temp": five_temp,"500 Sounding WD": five_wd,"500 Sounding WS": five_ws,
                          "200 Sounding GPH": two_gph,"200 Sounding Temp": two_temp,"200 Sounding WD": two_wd,"200 Sounding WS": two_ws}, 
                         index= date_index)
                        



raw_NAO_data = pd.read_csv(NAO_file_path, 
                           names = ("Year", "Month", "Day", "Data"), 
                           sep = " ", 
                           dtype = {"Data":np.float64}, 
                           skipinitialspace = True)

raw_PNA_data = pd.read_csv(PNA_file_path, 
                           names = ("Year", "Month", "Day", "Data"), 
                           sep = " ", 
                           dtype = {"Data":np.float64}, 
                           skipinitialspace = True)

raw_SOI_data = pd.read_csv(SOI_file_path, 
                           names = ("Date", "Data"), 
                           sep = ",", 
                           dtype = {"Data":np.float64})

raw_AO_data = pd.read_csv(AO_file_path, 
                           names = ("Date", "Data"), 
                           sep = ",", 
                           dtype = {"Data":np.float64})

raw_Station_data = pd.read_csv(Station_data_file_path)

print(raw_Station_data[["DATE"]])
Station_dates= pd.to_datetime(raw_Station_data["DATE"])


Station_data = pd.DataFrame(raw_Station_data[[Target_Value]].values,  columns = ["Target_Value"], index = Station_dates)

#print(Station_data.head)
#Station_dates = pd.DatetimeIndex(pd.to_datetime())

NAO_dates = pd.DatetimeIndex(pd.to_datetime(raw_NAO_data[["Year", "Month", "Day"]]))

NAO_data =pd.DataFrame(raw_NAO_data[["Data"]].values, columns = ["NAO"], index = NAO_dates)

PNA_dates = pd.DatetimeIndex(pd.to_datetime(raw_PNA_data[["Year", "Month", "Day"]]))

PNA_data =pd.DataFrame(raw_PNA_data[["Data"]].values, columns = ["PNA"], index = PNA_dates)

SOI_dates = pd.to_datetime(raw_SOI_data["Date"].values.flatten(), format = '%Y%m')

SOI_data = pd.DataFrame(raw_SOI_data[["Data"]].values, columns = ['SOI'], index = SOI_dates)


start_date = SOI_data.index.min()- pd.DateOffset(day=1)
end_date = SOI_data.index.max() + pd.DateOffset(day=31)
dates = pd.date_range(start_date, end_date, freq = 'D')

SOI_data = SOI_data.reindex(dates, method = 'ffill')
#print(SOI_data.head)

AO_dates = pd.to_datetime(raw_AO_data["Date"].values.flatten(), format = '%Y%m')

AO_data = pd.DataFrame(raw_AO_data[["Data"]].values, columns = ['AO'], index = AO_dates)

start_date = AO_data.index.min()- pd.DateOffset(day=1)
end_date = AO_data.index.max() +pd.DateOffset(day=31)
dates = pd.date_range(start_date, end_date, freq = 'D')

AO_data = AO_data.reindex(dates, method = 'ffill')


merged_data = PNA_data.merge(NAO_data, left_index = True, right_index = True)

merged_data = merged_data.merge(SOI_data, left_index = True, right_index = True)
merged_data = merged_data.merge(AO_data, left_index = True, right_index = True)
merged_data = merged_data.merge(Station_data, left_index = True, right_index=True)#, how = "left")
merged_data = merged_data.merge(sounding_entry(Sounding_filepath_1), left_index = True, right_index=True)
merged_data = merged_data.merge(sounding_entry(Sounding_filepath_2), left_index = True, right_index=True)
merged_data = merged_data.merge(sounding_entry(Sounding_filepath_5), left_index = True, right_index=True)
merged_data = merged_data.merge(sounding_entry(Sounding_filepath_6), left_index = True, right_index=True)
merged_data = merged_data.merge(sounding_entry(Sounding_filepath_7), left_index = True, right_index=True)
merged_data = merged_data.merge(sounding_entry(Sounding_filepath_8), left_index = True, right_index=True)
merged_data = merged_data.merge(sounding_entry(Sounding_filepath_9), left_index = True, right_index=True)
merged_data = merged_data.merge(sounding_entry(Sounding_filepath_10), left_index = True, right_index=True)
merged_data = merged_data.merge(sounding_entry(Sounding_filepath_11), left_index = True, right_index=True)
merged_data = merged_data.merge(sounding_entry(Sounding_filepath_13), left_index = True, right_index=True)
merged_data = merged_data.merge(sounding_entry(Sounding_filepath_15), left_index = True, right_index=True)
merged_data = merged_data.merge(sounding_entry(Sounding_filepath_16), left_index = True, right_index=True)

merged_data = merged_data.replace({-9999: np.NAN, -8888: np.NAN})
cleaned_data = merged_data.interpolate()
cleaned_data["MONTH"] = cleaned_data.index.month
cleaned_data["Day_of_Year"] = cleaned_data.index.day_of_year
cleaned_data[Target_Value +"_forecast"] = cleaned_data.shift(-target_forecast)["Target_Value"]
# cleaned_data["NAO_rate"] = (cleaned_data.shift(1)["NAO"] - cleaned_data.shift(-1)["NAO"])/2
# cleaned_data["AO_rate"] = (cleaned_data.shift(1)["AO"] - cleaned_data.shift(-1)["AO"])/2
# cleaned_data["PNA_rate"] = (cleaned_data.shift(1)["PNA"] - cleaned_data.shift(-1)["PNA"])/2
# cleaned_data["SOI_rate"] = (cleaned_data.shift(1)["SOI"] - cleaned_data.shift(-1)["SOI"])/2
# test_set, train_set = split_train(cleaned_data, .2)

cleaned_data.drop(index = cleaned_data.index[:target_forecast],axis = 0, inplace=True)
cleaned_data.drop(index = cleaned_data.index[-target_forecast:],axis = 0, inplace=True)


if not JJA:
    June = cleaned_data[cleaned_data["MONTH"] == 6]
    cleaned_data.drop(index = June.index, axis = 0, inplace= True)
    July = cleaned_data[cleaned_data["MONTH"] == 7]
    cleaned_data.drop(index = July.index, axis = 0, inplace= True)
    August = cleaned_data[cleaned_data["MONTH"] == 8]
    cleaned_data.drop(index = August.index, axis = 0, inplace= True)
if not SON:
    September = cleaned_data[cleaned_data["MONTH"] == 9]
    cleaned_data.drop(index = September.index, axis = 0, inplace= True)
    October = cleaned_data[cleaned_data["MONTH"] == 10]
    cleaned_data.drop(index = October.index, axis = 0, inplace= True)
    November = cleaned_data[cleaned_data["MONTH"] == 11]
    cleaned_data.drop(index = November.index, axis = 0, inplace= True)
if not DJF:
    December = cleaned_data[cleaned_data["MONTH"] == 12]
    cleaned_data.drop(index = December.index, axis = 0, inplace= True)
    January = cleaned_data[cleaned_data["MONTH"] == 1]
    cleaned_data.drop(index = January.index, axis = 0, inplace= True)
    February = cleaned_data[cleaned_data["MONTH"] == 2]
    cleaned_data.drop(index = February.index, axis = 0, inplace= True)
if not MAM:
    March = cleaned_data[cleaned_data["MONTH"] == 3]
    cleaned_data.drop(index = March.index, axis = 0, inplace= True)
    April = cleaned_data[cleaned_data["MONTH"] == 4]
    cleaned_data.drop(index = April.index, axis = 0, inplace= True)
    May = cleaned_data[cleaned_data["MONTH"] == 5]
    cleaned_data.drop(index = May.index, axis = 0, inplace= True)


model_labels = cleaned_data[Target_Value+"_forecast"].copy()
model_data = cleaned_data.copy()
model_data  = model_data.drop(Target_Value+"_forecast", axis=1) #for when predicting the current day temps based only on teleconnections


# model_data.to_pickle("./model_data.pkl")
# model_labels.to_pickle("./model_labels.pkl")

test_data = model_data.sample(frac = .1, random_state=42)
test_labels = model_labels.sample(frac = .1, random_state=42)
train_data = model_data.drop(test_data.index)
train_labels = model_labels.drop(test_labels.index)

print(model_data.shape)
print(test_data.shape)
print(test_labels.shape)
print(train_data.shape)
print(train_labels.shape)

train_data.to_pickle("./train_data.pkl")
train_labels.to_pickle("./train_labels.pkl")
test_data.to_pickle("./test_data.pkl")
test_labels.to_pickle("./test_labels.pkl")

##### NTS: Need to install Optional Dependency 'Tables' to use HDF5
# store_data = pd.HDFStore("Sounding_teleconnections.hdf5")
# store_data.put('S_T_data', model_data)
# metadata = {'Target Forecast': Target_Value, 'Forecast Length': target_forecast}
# store_data.get_storer('S_T_data').attrs.metadata = metadata



